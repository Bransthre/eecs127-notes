\contentsline {section}{\numberline {0.1}Preface}{2}{section.0.1}%
\contentsline {subsection}{\numberline {0.1.1}First of all, My Notes are Not Substitutes to the Course Readers}{2}{subsection.0.1.1}%
\contentsline {subsection}{\numberline {0.1.2}How did Brandon Use This Note?}{2}{subsection.0.1.2}%
\contentsline {subsection}{\numberline {0.1.3}What is the Last Four Digits of Your Social Security Number?}{2}{subsection.0.1.3}%
\contentsline {chapter}{\numberline {1}Introduction and Least Squares}{7}{chapter.1}%
\contentsline {section}{\numberline {1.1}An Introductory Prompt}{7}{section.1.1}%
\contentsline {section}{\numberline {1.2}Administratives, Summary}{7}{section.1.2}%
\contentsline {section}{\numberline {1.3}Introduction to The Subject Topic: Optimization}{7}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}An Example of Problem Formulation}{8}{subsection.1.3.1}%
\contentsline {section}{\numberline {1.4}Least Squares Regression}{9}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Formulating Least Squares Regression}{9}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Closed-Form Solution of Least Squares Problem}{9}{subsection.1.4.2}%
\contentsline {chapter}{\numberline {2} Linear Algebra Bootcamp: Norms, Gram-Schmidt, QR, FTLA }{11}{chapter.2}%
\contentsline {section}{\numberline {2.1}Vectors and Norms}{11}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}LP-Norms}{11}{subsection.2.1.1}%
\contentsline {section}{\numberline {2.2}Cauchy-Schwartz Inequality}{12}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Holder's Inequality and Norm Ball}{13}{subsection.2.2.1}%
\contentsline {section}{\numberline {2.3}Gram-Schmidt Orthonormalization and QR Decomposition}{14}{section.2.3}%
\contentsline {chapter}{\numberline {3}Linear Algebra: Symmetric Matrices}{16}{chapter.3}%
\contentsline {section}{\numberline {3.1}Fundamental Theorem of Linear Algebra}{16}{section.3.1}%
\contentsline {section}{\numberline {3.2}Minimum Norm Problem}{17}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Solution to the Minimum Norm Problem}{17}{subsection.3.2.1}%
\contentsline {section}{\numberline {3.3}Principal Component Analysis}{18}{section.3.3}%
\contentsline {chapter}{\numberline {4}Principal Component Analysis}{20}{chapter.4}%
\contentsline {section}{\numberline {4.1}Symmetric Matrix}{20}{section.4.1}%
\contentsline {section}{\numberline {4.2}PCA}{21}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Formulating PCA as an Optimization Problem}{22}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Solution to PCA as an Optimization Problem}{22}{subsection.4.2.2}%
\contentsline {chapter}{\numberline {5}SVD and Low-Rank Approximation}{24}{chapter.5}%
\contentsline {section}{\numberline {5.1}Singular Value Decomposition (SVD)}{24}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Formation of SVD}{25}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Geometry of SVD}{26}{subsection.5.1.2}%
\contentsline {section}{\numberline {5.2}Low Rank Approximation}{26}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Matrix Norms}{26}{subsection.5.2.1}%
\contentsline {chapter}{\numberline {6}Low-Rank Approximation}{29}{chapter.6}%
\contentsline {section}{\numberline {6.1}Discussion of L-RA}{29}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}The LRA Optimization Problem on Spectral Norm}{29}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}The LRA Optimization Problem on Frobenius Norm}{31}{subsection.6.1.2}%
\contentsline {chapter}{\numberline {7}Vector Calculus}{33}{chapter.7}%
\contentsline {section}{\numberline {7.1}Function Expansion: Taylor Series}{33}{section.7.1}%
\contentsline {section}{\numberline {7.2}Function Expansion: Derivative of Vector Functions}{34}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Examples of Polynomial Expansion}{34}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Matrix in Vector Calculus}{35}{subsection.7.2.2}%
\contentsline {chapter}{\numberline {8}The Extension of Vector Calculus}{38}{chapter.8}%
\contentsline {section}{\numberline {8.1}The Main Theorem}{38}{section.8.1}%
\contentsline {section}{\numberline {8.2}Perturbation Analysis, Effect of Noise}{39}{section.8.2}%
\contentsline {chapter}{\numberline {9}Ridge Regression}{41}{chapter.9}%
\contentsline {section}{\numberline {9.1}Perturbation Analysis Guides into Ridge Regression}{41}{section.9.1}%
\contentsline {section}{\numberline {9.2}Ridge Regression}{42}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}Development of Ridge Regression}{42}{subsection.9.2.1}%
\contentsline {section}{\numberline {9.3}Probabilistic Information from Ridge Regression}{43}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}Maximum Likelihood Estimation and Maximum A-Posteriori}{43}{subsection.9.3.1}%
\contentsline {subsection}{\numberline {9.3.2}A Personal Learning on MLE vs MAP}{45}{subsection.9.3.2}%
\contentsline {chapter}{\numberline {10}Convexity}{46}{chapter.10}%
\contentsline {section}{\numberline {10.1}Convex Set}{46}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}Proof of Convexity}{47}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}Hyperplanes}{48}{subsection.10.1.2}%
\contentsline {section}{\numberline {10.2}Convex Functions}{49}{section.10.2}%
\contentsline {chapter}{\numberline {11}Convex Optimization Problems}{50}{chapter.11}%
\contentsline {section}{\numberline {11.1}Convex Functions, Continued}{50}{section.11.1}%
\contentsline {section}{\numberline {11.2}Convex Optimization Problem}{52}{section.11.2}%
\contentsline {chapter}{\numberline {12}Descent Methods}{53}{chapter.12}%
\contentsline {section}{\numberline {12.1}Strict Strong Convexity}{53}{section.12.1}%
\contentsline {section}{\numberline {12.2}Gradient Descent}{54}{section.12.2}%
\contentsline {subsection}{\numberline {12.2.1}Inventing Gradient Descent}{54}{subsection.12.2.1}%
\contentsline {subsection}{\numberline {12.2.2}Finding $\eta $}{55}{subsection.12.2.2}%
\contentsline {subsection}{\numberline {12.2.3}Generalizations}{55}{subsection.12.2.3}%
\contentsline {chapter}{\numberline {13}Descent Methods and Convex Optimizations}{57}{chapter.13}%
\contentsline {section}{\numberline {13.1}Continued, Gradient Descent Convergence Proof}{57}{section.13.1}%
\contentsline {subsection}{\numberline {13.1.1}Breads of Quadratic Sandwich}{57}{subsection.13.1.1}%
\contentsline {subsection}{\numberline {13.1.2}Quadratic Sandwich}{58}{subsection.13.1.2}%
\contentsline {section}{\numberline {13.2}Stochastic Gradient Descent}{59}{section.13.2}%
\contentsline {chapter}{\numberline {14}Applications and Extensions of Gradient Descent}{61}{chapter.14}%
\contentsline {section}{\numberline {14.1}Stochastic Gradient Descent, Continued}{61}{section.14.1}%
\contentsline {subsection}{\numberline {14.1.1}A Demonstration of SGD}{61}{subsection.14.1.1}%
\contentsline {subsection}{\numberline {14.1.2}Mathematical Case Study on Convergence of SGD}{62}{subsection.14.1.2}%
\contentsline {section}{\numberline {14.2}Gradient Descent with Prior Optimization Constraint}{63}{section.14.2}%
\contentsline {subsection}{\numberline {14.2.1}Conditional Gradient Descent}{63}{subsection.14.2.1}%
\contentsline {chapter}{\numberline {15}Interlude: Logistic Regression}{64}{chapter.15}%
\contentsline {section}{\numberline {15.1}Monotone Transformations}{64}{section.15.1}%
\contentsline {subsection}{\numberline {15.1.1}Building Logistic Regression}{64}{subsection.15.1.1}%
\contentsline {chapter}{\numberline {16}Weak Duality}{66}{chapter.16}%
\contentsline {section}{\numberline {16.1}Active Constraints}{66}{section.16.1}%
\contentsline {section}{\numberline {16.2}Infimum vs. Minimum}{66}{section.16.2}%
\contentsline {section}{\numberline {16.3}Introduction to Lagrangian}{67}{section.16.3}%
\contentsline {subsection}{\numberline {16.3.1}Phrasing Optimization Problems}{67}{subsection.16.3.1}%
\contentsline {subsection}{\numberline {16.3.2}Lagrangian of an Optimization Problem}{67}{subsection.16.3.2}%
\contentsline {chapter}{\numberline {17}Weak, Strong Duality}{69}{chapter.17}%
\contentsline {section}{\numberline {17.1}Introduction to Weak vs. Strong Duality via Problems}{69}{section.17.1}%
\contentsline {subsection}{\numberline {17.1.1}Minimum-Norm Problem, Continued}{69}{subsection.17.1.1}%
\contentsline {subsection}{\numberline {17.1.2}Partitioning Problem}{70}{subsection.17.1.2}%
\contentsline {section}{\numberline {17.2}Minmax Inequality}{71}{section.17.2}%
\contentsline {chapter}{\numberline {18}Strong Duality and Optimality Conditions}{73}{chapter.18}%
\contentsline {section}{\numberline {18.1}Strong Duality}{73}{section.18.1}%
\contentsline {section}{\numberline {18.2}Optimality Conditions}{73}{section.18.2}%
\contentsline {section}{\numberline {18.3}Linear Program}{75}{section.18.3}%
\contentsline {chapter}{\numberline {19}Duality and Shadow Prices}{76}{chapter.19}%
\contentsline {section}{\numberline {19.1}A Review of KKT Condition}{76}{section.19.1}%
\contentsline {section}{\numberline {19.2}Dual of a Linear Program}{77}{section.19.2}%
\contentsline {section}{\numberline {19.3}Shadow Prices}{77}{section.19.3}%
\contentsline {chapter}{\numberline {20}Linear Programs}{79}{chapter.20}%
\contentsline {section}{\numberline {20.1}Mathematically Expressing Linear Programs}{79}{section.20.1}%
\contentsline {subsection}{\numberline {20.1.1}Translating General Forms into Standard Forms}{79}{subsection.20.1.1}%
\contentsline {section}{\numberline {20.2}Interior Point}{81}{section.20.2}%
\contentsline {chapter}{\numberline {21}Quadratic Program}{83}{chapter.21}%
\contentsline {section}{\numberline {21.1}Linear Programs, continued}{83}{section.21.1}%
\contentsline {section}{\numberline {21.2}Quadratic Programs}{84}{section.21.2}%
\contentsline {subsection}{\numberline {21.2.1}Solving an Unconstrained QP}{84}{subsection.21.2.1}%
\contentsline {chapter}{\numberline {22}More on Quadratic Programs, and SOCPs}{85}{chapter.22}%
\contentsline {section}{\numberline {22.1}More on Quadratic Programs}{85}{section.22.1}%
\contentsline {subsection}{\numberline {22.1.1}Categories of Quadratic Program}{85}{subsection.22.1.1}%
\contentsline {subsection}{\numberline {22.1.2}Applications of QP}{86}{subsection.22.1.2}%
\contentsline {subsection}{\numberline {22.1.3}Another Example of Relaxing an Optimization Problem}{86}{subsection.22.1.3}%
\contentsline {section}{\numberline {22.2}Second Order Cone Programs (SOCP)}{87}{section.22.2}%
\contentsline {subsection}{\numberline {22.2.1}Facility Location Problem}{88}{subsection.22.2.1}%
\contentsline {chapter}{\numberline {23}LASSO}{89}{chapter.23}%
\contentsline {section}{\numberline {23.1}L1-norm Optimization}{89}{section.23.1}%
\contentsline {subsection}{\numberline {23.1.1}L1 Least Squares}{90}{subsection.23.1.1}%
\contentsline {section}{\numberline {23.2}LASSO (L1) Regularization}{90}{section.23.2}%
\contentsline {subsection}{\numberline {23.2.1}A Demonstration of LASSO in Scalar Case}{91}{subsection.23.2.1}%
\contentsline {chapter}{\numberline {24}Descent Methods}{92}{chapter.24}%
\contentsline {section}{\numberline {24.1}Coordinate Descent}{92}{section.24.1}%
\contentsline {subsection}{\numberline {24.1.1}The Descent of LASSO}{92}{subsection.24.1.1}%
\contentsline {section}{\numberline {24.2}Newton's Method}{93}{section.24.2}%
\contentsline {subsection}{\numberline {24.2.1}Equality Constrained Quadratic Program}{94}{subsection.24.2.1}%
\contentsline {chapter}{\numberline {25}Optimization Applications I: LQR (Linear-Quadratic Regulator) Control}{95}{chapter.25}%
\contentsline {section}{\numberline {25.1}Introduction to Model-Based Control}{95}{section.25.1}%
\contentsline {section}{\numberline {25.2}LQR Control Via Duality and Certificate}{95}{section.25.2}%
\contentsline {chapter}{\numberline {26}Support Vector Machine}{97}{chapter.26}%
\contentsline {section}{\numberline {26.1}Introduction to Classifiers}{97}{section.26.1}%
\contentsline {section}{\numberline {26.2}SVM as an Optimization Problem}{97}{section.26.2}%
\contentsline {subsection}{\numberline {26.2.1}Hard-Margin SVM}{97}{subsection.26.2.1}%
\contentsline {subsection}{\numberline {26.2.2}Soft-Margin SVM}{99}{subsection.26.2.2}%
\contentsline {subsection}{\numberline {26.2.3}Support Vector Identification via Duality}{99}{subsection.26.2.3}%
\contentsline {section}{\numberline {26.3}Computing the Dual of SVM}{100}{section.26.3}%
\contentsline {section}{\numberline {26.4}Kernel Trick}{101}{section.26.4}%
\contentsline {section}{\numberline {26.5}Kernel Trick (from COMPSCI 189)}{101}{section.26.5}%
\contentsline {subsection}{\numberline {26.5.1}Motivation}{101}{subsection.26.5.1}%
\contentsline {subsection}{\numberline {26.5.2}Kernel Ridge Regression}{102}{subsection.26.5.2}%
\contentsline {subsection}{\numberline {26.5.3}The Kernel Trick (Kernelization), Polynomial}{102}{subsection.26.5.3}%
\contentsline {chapter}{\numberline {27}Interior Point Methods}{104}{chapter.27}%
\contentsline {section}{\numberline {27.1}Introduction to Interior Point Method}{104}{section.27.1}%
\contentsline {section}{\numberline {27.2}Barrier Function}{104}{section.27.2}%
\contentsline {chapter}{\numberline {28}Mixed: Integer Programming}{106}{chapter.28}%
\contentsline {section}{\numberline {28.1}Integer Programming}{106}{section.28.1}%
\contentsline {subsection}{\numberline {28.1.1}Exploration by Example}{106}{subsection.28.1.1}%
\contentsline {section}{\numberline {28.2}Disjunction of Constraints}{107}{section.28.2}%
\contentsline {section}{\numberline {28.3}Algorithmic Works}{107}{section.28.3}%
\contentsline {subsection}{\numberline {28.3.1}Demonstration of BB Algorithm}{108}{subsection.28.3.1}%
\contentsline {chapter}{\numberline {29}Revision Log}{109}{chapter.29}%
