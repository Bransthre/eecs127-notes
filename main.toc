\contentsline {section}{\numberline {0.1}Preface}{2}{section.0.1}%
\contentsline {subsection}{\numberline {0.1.1}First of all, My Notes are Not Substitutes to the Course Readers}{2}{subsection.0.1.1}%
\contentsline {subsection}{\numberline {0.1.2}How did Brandon Use This Note?}{2}{subsection.0.1.2}%
\contentsline {subsection}{\numberline {0.1.3}What is the Last Four Digits of Your Social Security Number?}{2}{subsection.0.1.3}%
\contentsline {chapter}{\numberline {1}Introduction and Least Squares}{5}{chapter.1}%
\contentsline {section}{\numberline {1.1}An Introductory Prompt}{5}{section.1.1}%
\contentsline {section}{\numberline {1.2}Administratives, Summary}{5}{section.1.2}%
\contentsline {section}{\numberline {1.3}Introduction to The Subject Topic: Optimization}{5}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}An Example of Problem Formulation}{6}{subsection.1.3.1}%
\contentsline {section}{\numberline {1.4}Least Squares Regression}{7}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Formulating Least Squares Regression}{7}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}Closed-Form Solution of Least Squares Problem}{7}{subsection.1.4.2}%
\contentsline {chapter}{\numberline {2} Linear Algebra Bootcamp: Norms, Gram-Schmidt, QR, FTLA }{9}{chapter.2}%
\contentsline {section}{\numberline {2.1}Vectors and Norms}{9}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}LP-Norms}{9}{subsection.2.1.1}%
\contentsline {section}{\numberline {2.2}Cauchy-Schwartz Inequality}{10}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}Holder's Inequality and Norm Ball}{11}{subsection.2.2.1}%
\contentsline {section}{\numberline {2.3}Gram-Schmidt Orthonormalization and QR Decomposition}{12}{section.2.3}%
\contentsline {chapter}{\numberline {3}Linear Algebra: Symmetric Matrices}{14}{chapter.3}%
\contentsline {section}{\numberline {3.1}Fundamental Theorem of Linear Algebra}{14}{section.3.1}%
\contentsline {section}{\numberline {3.2}Minimum Norm Problem}{15}{section.3.2}%
\contentsline {subsection}{\numberline {3.2.1}Solution to the Minimum Norm Problem}{15}{subsection.3.2.1}%
\contentsline {section}{\numberline {3.3}Principal Component Analysis}{16}{section.3.3}%
\contentsline {chapter}{\numberline {4}Principal Component Analysis}{18}{chapter.4}%
\contentsline {section}{\numberline {4.1}Symmetric Matrix}{18}{section.4.1}%
\contentsline {section}{\numberline {4.2}PCA}{19}{section.4.2}%
\contentsline {subsection}{\numberline {4.2.1}Formulating PCA as an Optimization Problem}{20}{subsection.4.2.1}%
\contentsline {subsection}{\numberline {4.2.2}Solution to PCA as an Optimization Problem}{20}{subsection.4.2.2}%
\contentsline {chapter}{\numberline {5}SVD and Low-Rank Approximation}{22}{chapter.5}%
\contentsline {section}{\numberline {5.1}Singular Value Decomposition (SVD)}{22}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}Formation of SVD}{23}{subsection.5.1.1}%
\contentsline {subsection}{\numberline {5.1.2}Geometry of SVD}{24}{subsection.5.1.2}%
\contentsline {section}{\numberline {5.2}Low Rank Approximation}{24}{section.5.2}%
\contentsline {subsection}{\numberline {5.2.1}Matrix Norms}{24}{subsection.5.2.1}%
\contentsline {chapter}{\numberline {6}Low-Rank Approximation}{27}{chapter.6}%
\contentsline {section}{\numberline {6.1}Discussion of L-RA}{27}{section.6.1}%
\contentsline {subsection}{\numberline {6.1.1}The LRA Optimization Problem on Spectral Norm}{27}{subsection.6.1.1}%
\contentsline {subsection}{\numberline {6.1.2}The LRA Optimization Problem on Frobenius Norm}{29}{subsection.6.1.2}%
\contentsline {chapter}{\numberline {7}Vector Calculus}{31}{chapter.7}%
\contentsline {section}{\numberline {7.1}Function Expansion: Taylor Series}{31}{section.7.1}%
\contentsline {section}{\numberline {7.2}Function Expansion: Derivative of Vector Functions}{32}{section.7.2}%
\contentsline {subsection}{\numberline {7.2.1}Examples of Polynomial Expansion}{32}{subsection.7.2.1}%
\contentsline {subsection}{\numberline {7.2.2}Matrix in Vector Calculus}{33}{subsection.7.2.2}%
\contentsline {chapter}{\numberline {8}The Extension of Vector Calculus}{36}{chapter.8}%
\contentsline {section}{\numberline {8.1}The Main Theorem}{36}{section.8.1}%
\contentsline {section}{\numberline {8.2}Perturbation Analysis, Effect of Noise}{37}{section.8.2}%
\contentsline {chapter}{\numberline {9}Ridge Regression}{39}{chapter.9}%
\contentsline {section}{\numberline {9.1}Perturbation Analysis Guides into Ridge Regression}{39}{section.9.1}%
\contentsline {section}{\numberline {9.2}Ridge Regression}{40}{section.9.2}%
\contentsline {subsection}{\numberline {9.2.1}Development of Ridge Regression}{40}{subsection.9.2.1}%
\contentsline {section}{\numberline {9.3}Probabilistic Information from Ridge Regression}{41}{section.9.3}%
\contentsline {subsection}{\numberline {9.3.1}Maximum Likelihood Estimation and Maximum A-Posteriori}{41}{subsection.9.3.1}%
\contentsline {subsection}{\numberline {9.3.2}A Personal Learning on MLE vs MAP}{43}{subsection.9.3.2}%
\contentsline {chapter}{\numberline {10}Convexity}{44}{chapter.10}%
\contentsline {section}{\numberline {10.1}Convex Set}{44}{section.10.1}%
\contentsline {subsection}{\numberline {10.1.1}Proof of Convexity}{45}{subsection.10.1.1}%
\contentsline {subsection}{\numberline {10.1.2}Hyperplanes}{46}{subsection.10.1.2}%
\contentsline {section}{\numberline {10.2}Convex Functions}{47}{section.10.2}%
\contentsline {chapter}{\numberline {11}Convex Optimization Problems}{48}{chapter.11}%
\contentsline {section}{\numberline {11.1}Convex Functions, Continued}{48}{section.11.1}%
\contentsline {section}{\numberline {11.2}Convex Optimization Problem}{50}{section.11.2}%
\contentsline {chapter}{\numberline {12}Descent Methods}{51}{chapter.12}%
\contentsline {section}{\numberline {12.1}Strict Strong Convexity}{51}{section.12.1}%
\contentsline {section}{\numberline {12.2}Gradient Descent}{52}{section.12.2}%
\contentsline {subsection}{\numberline {12.2.1}Inventing Gradient Descent}{52}{subsection.12.2.1}%
\contentsline {subsection}{\numberline {12.2.2}Finding $\eta $}{53}{subsection.12.2.2}%
\contentsline {subsection}{\numberline {12.2.3}Generalizations}{53}{subsection.12.2.3}%
\contentsline {chapter}{\numberline {13}Descent Methods and Convex Optimizations}{55}{chapter.13}%
\contentsline {section}{\numberline {13.1}Continued, Gradient Descent Convergence Proof}{55}{section.13.1}%
\contentsline {subsection}{\numberline {13.1.1}Breads of Quadratic Sandwich}{55}{subsection.13.1.1}%
\contentsline {subsection}{\numberline {13.1.2}Quadratic Sandwich}{56}{subsection.13.1.2}%
\contentsline {section}{\numberline {13.2}Stochastic Gradient Descent}{57}{section.13.2}%
