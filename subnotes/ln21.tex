\chapter{Quadratic Program}

\section{Quadratic Programs}
\begin{ln-define}{Quadratic Program}{}
    A quadratic program is an optimization problem with a quadratic objective function and linear constraints.
    \tcblower
    QPs (quadratic programs) are genreally phrased as:
    \[
        \min_{
            \substack{
                A \vec{x} \leq \vec{b} \\
                F \vec{x} = \vec{d}
            }
        } \frac{1}{2} \vec{x}^T H \vec{x} + \vec{c}^T \vec{x}
    \]
    Conventionally, we have $H$ be symmetric, and $H$ need be positive-semidefinite to guarantee convexity.
\end{ln-define}

\subsection{Solving an Unconstrained QP}
Consider the quadratic program:
\[
    p^* = \min_{\vec{x} \in \R^n} \frac{1}{2} \vec{x}^T H \vec{x} + \vec{c} ^T \vec{x} + d
\]
\textbf{Case 1.} Suppose one case where,
\[
    H \succcurlyeq 0, \vec{c} \in \mathcal{R}(H)
\]
\begin{quote}
    Then, let $H \vec{x}_0 = -\vec{c}$, and keep in mind that $H$ is summetric.
    \begin{align*}
        f(\vec{x})
        &= \frac{1}{2} {(\vec{x} - \vec{x}_0)}^T H (\vec{x} - \vec{x}_0) + \alpha \\
        &= \frac{1}{2} \vec{x}^T H \vec{x} + \frac{1}{2} \vec{x}_0^T H \vec{x}_0 - \vec{x}_0^T H \vec{x} + \alpha
    \end{align*}
    Then, we may figure that the minimizing argument of this objective function is $\vec{x}_0$ and $p^* = \alpha$. \\
    Then, suppose $H$ is invertible, we find that $\vec{x}_0 = - H^{-1} \vec{c}$
\end{quote}
\textbf{Case 2.} Suppose one other case where,
\[
    H \succcurlyeq 0, \vec{c} \not\in \mathcal{R}(H)
\]
\begin{quote}
    In such case, we may phrase that $\vec{c} = -H \vec{x_0} + \vec{r}$, where $\vec{r} \in {\mathcal{R}(H)}^{\perp} = N(H^T) = N(H)$ by the Fundamental Theorem of Linear Algebra. \\
    Then,
    \begin{align*}
        f(k \vec{r})
        &= \frac{1}{2} {(k \vec{r})}^T H (k \vec{r}) + \vec{c}^T (k \vec{r}) + d \\
        &= 0 + {(-H \vec{x}_0 + \vec{r})}^T k \vec{r} + d \\
        &= k \pnorm[2]{\vec{r}}{2} - k \vec{x_0}^T H^T \vec{r} + d \\
        &= k \pnorm[2]{\vec{r}}{2} + d \\
        \lim_{k \rightarrow -\infty} f(k \vec{r}) &= -\infty
    \end{align*}
\end{quote}
\textbf{Case 3.} Suppose one last case where, there exists a negative eigenvalue in $H$.
\begin{quote}
    Then, we may use a negative eigenvalue of $H$ to lead its minimum to $-\infty$.
\end{quote}

\section{Categories of Quadratic Program}
Upon the above explorations, let us formally define an unconstrained quadratic program:
\begin{ln-define}{Unconstrained Quadratic Programs}{}
    An unconstrained QP is generally stated as:
    \[
        \min_{A \vec{x} \leq \vec{b}} \frac{1}{2} \vec{x}^T H \vec{x} + \vec{c}^T \vec{x} + \vec{d}
    \]
    where, since this is a QP, we have a quadratic objective function and linear constraints. \\
    For any QP with a convex objective function, the optimization problem itself is convex.
\end{ln-define}
Furthermore, we discovered that, in the above definition,
\begin{quote}
    If $H \succcurlyeq 0$ and $\vec{c} \in \mathcal{R}(H)$, then we may have an explicit solution.
    Else, the primal solution of this QP is $p^* = -\infty$.
\end{quote}
Meanwhile, a constrained QP may be defined as follows:
\begin{ln-define}{Constrained Quadratic Program}{}
    A constrained QP is generally expressed as:
    \[
        \min_{A \vec{x} = \vec{b}} \frac{1}{2} \vec{x}^T H \vec{x} + \vec{c}^T \vec{x} + \vec{d}
    \]
\end{ln-define}
For a constrained QP:
\[
    \min_{A \vec{x} = \vec{b}} \frac{1}{2} \vec{x}^T H \vec{x} + \vec{c}^T \vec{x} + \vec{d}
\]
Let us collect an idea from the previosu section for decomposing the feasible set.
Let $N$ is the basis for $\mathcal{N}(A)$, the nullspace of $A$.
Then, for $\vec{x}_0$ is some solution of the optimization problem, the feasible set is expressed as
\[
    \{
        \vec{x} | \vec{x} = \vec{x}_0 + N \vec{z}
    \}
\]
Therefore, we may rewrite the constrained QP as:
\begin{align*}
    &\min_{
        \substack{
            \vec{x} = \vec{x}_0 + N \vec{z} \\
            \vec{z}
        }
    } \frac{1}{2} {(\vec{x}_0 + N \vec{z})}^T H (\vec{x}_0 + N \vec{z}) + \vec{c}^T (\vec{x}_0 + N \vec{z}) + d \\
    &=
    \min_{\vec{z}} \frac{1}{2} \vec{z}^T (N^T H N) \vec{z} + (\vec{x}_0^T H N + \vec{c}^T N) \vec{z} + \frac{1}{2} \vec{x}_0^T H \vec{x}_0 + \vec{c}^T \vec{x}_0 + d
\end{align*}
which is an unconstrained QP.

\section{Applications of QP}
\textbf{LQR.} One famous application of Quadratic Programs is the Linear-Quadratic Regulator (LQR) Problem, where
\begin{quote}
    Provided some model $\vec{x}(t + 1) = A \vec{x}(t) + B \vec{u}(t)$, we minimize the cost of operation:
    \[
        \min \sum_{t = 0}^T \pnorm[2]{\vec{u}(t)}{2} + \pnorm[2]{\vec{x}(T) - \vec{g}}{2}
    \]
\end{quote}

\textbf{LASSO.} Another famous application of Quadratic Program is Least Squares (and Ridge Regression) Algorithm (an unconstrained quadratic program). \\
And notably, the LASSO regularization,
\[
    \min_{\vec{x}} \pnorm[2]{A \vec{x} - \vec{b}}{2} + \lambda \pnorm{\vec{x}}{1}
\]
is also a quadratic program, as it can be rephrased into a problem of two variables $x_i$ and $t_i = |x_i|$. \\
\[
    \min_{
        \substack{
            \vec{x}, \vec{t} \\
            |x_i| = t_i \\
            t_i \geq 0
        }
    } \pnorm[2]{A \vec{x} - \vec{b}}{2} + \lambda \sum_{i = 1}^n t_i
\]
and then relaxed as:
\[
    \min_{
        \substack{
            \vec{x}, \vec{t} \\
            \forall i, -t_i \leq x \leq t_i \\
            t_i \geq 0
        }
    } \pnorm[2]{A \vec{x} - \vec{b}}{2} + \lambda \sum_{i = 1}^n t_i
\]
Why does this relaxation preserve the original minimum? \\
Suppose that for some $i$, $\exists \epsilon > 0, t_i = x_i + \epsilon$, such that we have some nonzero slack, and find that another better optimal exists at ${t_i}' = t_i - \epsilon$ where the regularizing term becomes smaller. \\
However, we would in turn find a better optimum than $x$. Therefore, it must be that the true optimum exists where there is zero slack: where the pre-relaxed constraints hold.

\textbf{Piecewise Linear Filling Problem}
Let us look at the piecewise linear filling problem, where we observe that for a set $\{y_1, \dots, y_n\}$ and piecewise constants $\{x_1, \dots, x_n\}$,
\[
    \forall i, y_i = x_i + noise
\]
The problem deamnds us to find $\hat{\vec{x}}$ such that $\hat{\vec{x}}$ does not change on consecutive timesteps, and is a good piecewise estimation of $\vec{y}$.

To solve this problem, let us define a matrix
\[
    D =
    \begin{bmatrix}
        -1 & 1 & 0 & \cdots & 0 \\
        0 & -1 & 1 & \cdots & 0 \\
        \vdots & \ddots & \ddots & \ddots & \vdots \\
        0 & \cdots & 0 & -1 & 1
    \end{bmatrix}
\]
such that,
\[
    -D \vec{x} =
    \begin{bmatrix}
        x_2 - x_1 \\
        x_3 - x_2 \\
        \vdots
        x_n - x_{n - 1}
    \end{bmatrix}
\]
and we would like that the cardinality of $D \vec{x}$ is $Card(D \vec{x}) \leq k$. \\
Therefore, we may define the Piecewise Linear Filling problem as:
\[
    \min_{card(D \hat{\vec{x}}) \leq k}
    \pnorm[2]{\vec{y} - \hat{\vec{x}}}{2}
\]

We will then relax the problem to have a L1-norm cosntraint instead of a relaxed one:
\[
    \min_{\pnorm{\vec{x}}{1} \leq k}
    \pnorm[2]{\vec{y} - \hat{\vec{x}}}{2}
\]
A class of methods is proficient at solving these problems efficiently, called the Interior Point Methods.

