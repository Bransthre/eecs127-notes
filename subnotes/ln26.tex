\chapter{Support Vector Machine}
This note contains two lectures worth of content, and is put in one chapter for coherency.

\section{Introduction to Classifiers}
In the scheme of classification vectors, we attempt to classify some datapoint $(\vec{x}, y)$ among specific classes, where we attempt to predict the label of this datapoint, $y \in \{1, -1\}$, based on its feature vector, $\vec{x}$.
Support Vector Machine is a classification algorithm.
Unlike logistic regression, which offers the probability that a point belonging to a class, SVM is an algorithm that directly provides the prediction of a datapoint based on geometric properties. \\
This geometric property we attempt to find is ``separating hyperplane'', where we find a hyperplane that can separate each class of points on one side of the hyperplane.

Remember that this hyperplane, which becomes the ``decision boundary'' of our classifier (because it is the boundary upon which prediction is decided) can be written as:
\[
    f(\vec{x}) = \vec{w}^T \vec{x} + b = \vec{w}^T (\vec{x} + \vec{x}_0)
\]
Therefore, our prediction rule becomes:
\[
    y_i = \begin{cases}
        1 &, f(\vec{x}_i) > 0 \\
        -1 &, f(\vec{x}_i) < 0
    \end{cases}
\]
Compactly rewriting the above expression, we find that we actually require
\[
    y_i f(\vec{x}_i) > 0
\]

\section{SVM as an Optimization Problem}
If our dataset can be separated by a hyperplane in such manner, we call our data ``linearly separable''. \\
However, there can exist many hyperplanes to result in a linear separation of some dataset.
So which hyperplane is better?
Per convention of optimization problem, we can come up with a heuristic that rates how good each hyperplane works as a decision boundary.

\subsection{Hard-Margin SVM}
Conceptually, to permit for noise perturbations in datapoints, we would like our decision boundary to have as much distance from each point as possible.
Here we define this concept as:
\begin{ln-define}{Margin}{}
    \textbf{Margin} is the distance between a hyperplane and the closest datapoint to it.
\end{ln-define}
We would like our decision boundary to have the maximum margin. This result is called a \textbf{max margin separator}.

What is the distance from a point to a hyperplane? \\
Fortunately, we have done a similar analysis in the most recent CSM16A worksheet. Let me shamelessly quote a section of that (which is still written by myself):
\begin{ln-explain}{Distance from Point to Hyperplane}{}
    \textbf{Problem Statement.} Provided a point $X$ and a hyperplane $\mathcal{H} = \{\vec{x} \big| \vec{w}^T \vec{x} + b = 0\}$, what is the distance between $X$ and $\mathcal{H}$?
    \tcblower
    \textbf{Solution.}    
    Let $\vec{x}$ represent the point $X$, and $\vec{y}$ be ${proj}_{\mathcal{H}}(\vec{x})$. \\
    Then, such difference should also be a multiple of $\vec{w}$ as well, which is orthogonal to $\mathcal{H}$, such that:
    \[
        (\vec{x} - \vec{y}) = \alpha \vec{w}
    \]
    Therefore, the arithmetic follows:
    \begin{align*}
        (\vec{x} - \vec{y}) &= \alpha \vec{w} \\
        (\vec{x} - \vec{y}) \cdot \vec{w} &= \alpha \vec{w} \cdot \vec{w} \\
        \vec{y} \in \mathcal{H} &\implies \vec{y} \cdot \vec{w} + b = 0 \\
        \vec{x} \cdot \vec{w} + b &= \alpha \vec{w} \cdot \vec{w} \\
        \alpha &= \frac{\vec{w}^T \vec{x} + b}{\pnorm[2]{\vec{w}}{2}}
    \end{align*}
    Then, the shortest distance between $X$ and $\mathcal{H}$ may be expressed as:
    \[
        \pnorm{\vec{x} - \vec{y}}{2} = \alpha \pnorm{\vec{w}}{2} = \frac{\vec{w}^T \vec{x} + b}{\pnorm{\vec{w}}{2}}
    \]
\end{ln-explain}

The optimization problem for finding the maximal margin separator would thus be:
\[
    \max_{
        \substack{
            \vec{w}, b, m \\
            \forall i, \frac{\vec{w}^T \vec{x}_i + b}{\pnorm{\vec{w}}{2}} \geq m \\
            \forall i, y_i (\vec{w}^T \vec{x}_i + b) > 0
        }
    }
    m
\]
This problem is known as the ``Hard-Margin SVM Problem'', more popularly formulated as:
\[
    \max_{
        \substack{
            \vec{w}, b, m \\
            \forall i, \frac{y_i (\vec{w}^T \vec{x}_i + b)}{\pnorm{\vec{w}}{2}} \geq m \\
            m \geq 0
        }
    }
    m
\]
To let the problem have a unique solution, we may normalize $\vec{w}$ by choosing $\frac{1}{\pnorm{\vec{w}}{2}} = m$. \\
This causes our problem to be further reduced as we eliminate $m$ and change our maximization problem into an equivalent minimization problem:
\[
    \min_{
        \substack{
            \vec{w}, b \\
            \forall i, y_i (\vec{w}^T \vec{x}_i + b) \geq 1
        }
    }
    \frac{1}{2} \pnorm[2]{\vec{w}}{2}
\]
This is a Quadratic Program.

\subsection{Soft-Margin SVM}
In soft-margin SVM, we allow for margin violation, such that the problem is rephrased as:
\[
    \min_{
        \substack{
            \vec{w}, b \\
            \forall i, y_i (\vec{w}^T \vec{x}_i + b) \geq 1 - \xi_i \\
            \forall i, \xi_i \geq 0
        }
    }
    \frac{1}{2} \pnorm[2]{\vec{w}}{2}
\]
with slack variables $\xi_i$. \\
However, notice that we would want $\xi_i$ to be small as well. \\
Therefore, we should regularize our obejctive function with some norm of $\vec{\xi}$:
\[
    \min_{
        \substack{
            \vec{w}, b, \vec{\xi} \\
            \forall i, y_i (\vec{w}^T \vec{x}_i + b) \geq 1 - \xi_i \\
            \forall i, \xi_i \geq 0
        }
    }
    \frac{1}{2} \pnorm[2]{\vec{w}}{2} + C \sum_{i = 1}^n \xi_i
\]
where $C > 0$ and controls the strength of regularization just as $\lambda$ does in LASSO Regression:
\begin{bindenum}
    \item Small $C$: Less Sensitive to Margin Violation
    \item Large $C$: More Sensitive to Margin Violation
\end{bindenum}

Now we may also consider the loss function of SVM. \\
Suppose we illustrate our loss function to be a $0-1$ loss based on mispredictions, then we would be facing a non-convex loss function.
We should then attempt on the convexization of such function, resulting in a function that we call \textbf{hinge loss}:
\[
    L_{\rm hinge} = \max(1 - y_i (\vec{w}^T \vec{x}_i + b), 0)
\]
Furthermore, combining the two optimization problem constraints above does grant us that:
\[
    \forall i, \xi_i \geq \max(1 - y_i (\vec{w}^T \vec{x}_i + b), 0)
\]
Therefore, our formulation for soft-margin SVM can be further paraphrased as its \textbf{Hinge-Loss Formulation}:
\[
    \min_{
        \substack{
            \vec{w}, b \\
            \forall i, y_i (\vec{w}^T \vec{x}_i + b) \geq 1 - \xi_i \\
            \forall i, \xi_i \geq 0
        }
    }
    \frac{1}{2} \pnorm[2]{\vec{w}}{2} + C \sum_{i = 1}^n L_{\rm hinge} (y_i, \vec{w}^T \vec{x}_i + b)
\]

\subsection{Support Vector Identification via Duality}
Suppose that we let the dual variables $\alpha_i$, $\beta_i$ correspond to the soft margin SVM formulation's first and second constraint. \\
Then, we obtain a Lagrangian:
\begin{align*}
    \mathcal{L} &(\vec{w}, b, \vec{\xi}, \vec{\alpha}, \beta) \\
    &= \frac{1}{2} \pnorm[2]{\vec{w}}{2} + c \sum_{i = 1}^n \xi_i + \sum_{i = 1}^n \alpha_i \big( (1 - \xi_i) - y_i (\vec{w}^T \vec{x}_i + b) \big) \\
    &= \frac{1}{2} \pnorm[2]{\vec{w}}{2} - \sum_{i=1}^n \alpha_i y_i (\vec{w}^T \vec{x}_i + b) + \sum_{i = 1}^n \alpha_i + \sum_{i = 1}^n (C - \alpha_i - \beta_i) \xi_i
\end{align*}
The KKT Condition becomes necessary and sufficient for strong duality to hold, as we have convex, aggine constraint with a feasible set of possible solutions (a more precise version of Slater's Condition will not be readdressed here).

Then, the KKT Conditions should be as quoted in the following:
\begin{quote}
    \textbf{First-Order Conditions:}
    \[
        \begin{cases}
            \nabla_{\vec{w}} \mathcal{L} = \vec{w} - \sum_i \alpha_i y_i \vec{x}_i = 0 \\
            \pdv{\mathcal{L}}{b} = -\sum_i \alpha_i y_i = 0 \\
            \pdv{\mathcal{L}}{\xi_i} = C - \alpha_i - \beta_i = 0
        \end{cases}
    \]
\end{quote}
From which we can already extract insights on:
\begin{bindenum}
    \item The expression of $\vec{w}$ and $C$
    \item Some $\alpha_i$ becoming non-zero shows its respective optimization constraint to be active.
\end{bindenum}
\begin{quote}
    \textbf{Complementary Slackness:}
    \[
        \begin{cases}
            \alpha_i \big( (1 - \xi_i) - y_i (\vec{w}^T \vec{x}_i + b) \big) = 0 \\
            \beta_i \xi_i = 0
        \end{cases}
    \]
\end{quote}
Here, if $0 < \alpha_i  < C$ (such that it is nonzero), then $\big( (1 - \xi_i) - y_i (\vec{w}^T \vec{x}_i + b) \big) = 0$.
Therefore, $\alpha_i$ desides which datapoints are the support vectors of a dataset, in that any non-zero $\alpha_i$ indicates its corresponding point to have an active constraint. \\
Meanwhile, $\beta_i$ decides the non-zero-ness of slack variables, such that upon the first-order KKT condition, $\beta_i$ not being $0$ informs us that $\psi_i$ is $0$.
Hence:
\[
    (0 < \alpha_i < C \land \beta_i \neq 0) \implies y_i (\vec{w}^T \vec{x}_i + b) = 1
\]
such vector that lies on the margin is known as a support vector. \\
On the other hand, having a $\beta_i$ provides that such vector is a support vector, but unknown whether on the margin or not.
In that regard, any vector that forms an active constraint is a support vector. \\
A vector with $\alpha_i = 0$ is not a support vector then; we may observe this by the KKT condition, and see that any point with $\alpha_i = 0$ does not contribute to the formulation of $\vec{w}$.
While they can be on the margin, they are ``coincidentally'' on the margin and do not themselves decide the margin.

\section{Computing the Dual of SVM}
Because Slater's Condition holds for the dual problem of SVM, strong duality holds. \\
Let us then consider that our solution obeys the KKT Conditions listed in the previous section, such that
\begin{align*}
    \mathcal{L}_{\rm KKT}(\vec{w}, \vec{b}, \vec{\xi}, \vec{\alpha}, \vec{\beta})
    &= \frac{1}{2} \pnorm[2]{\vec{w}}{2} - \sum_i \alpha_i y_i \vec{w}^T \vec{x}_i + \sum_i \alpha_i \\
    &= \frac{1}{2} \pnorm[2]{\vec{w}}{2} - \vec{w}^T \sum_i \alpha_i y_i \vec{x}_i + \sum_i \alpha_i \\
    &= \frac{1}{2} \pnorm[2]{\vec{w}}{2} - \pnorm[2]{\vec{w}}{2} + \sum_i \alpha_i \\
    &= - \frac{1}{2} \pnorm[2]{\vec{w}}{2} + \sum_i \alpha_i \\
    &= - \frac{1}{2} {\bigg( \sum_i \alpha_i y_i \vec{x}_i \bigg)}^T {\bigg( \sum_i \alpha_i y_i \vec{x}_i \bigg)} + \sum_i \alpha_i \\
    &= - \frac{1}{2} \vec{\alpha}^T {\rm diag}(\vec{y}) X X^T {\rm diag}(\vec{y}) \vec{\alpha} + \sum_i \alpha_i \\
    &= - \frac{1}{2} \vec{\alpha}^T Q \vec{\alpha} + \sum_i \alpha_i \\
    Q &= {\rm diag}(\vec{y}) X X^T {\rm diag}(\vec{y})
\end{align*}
where
\[
    X = \begin{bmatrix} \vec{x}_1^T \\ \vdots \\ \vec{x}_n^T \end{bmatrix}
\]
is out design matrix. \\
Then, to solve for the max-min of this KKT-substituted Lagrangian, we may perform the following procedure:
\[
    d^* = \max_{
        \substack{
            \vec{\alpha}, \vec{\beta} \\
            \sum \alpha_i y_i = 0 \\
            \forall i, C - \alpha_i - \beta_i = 0 \\
            \forall i, \alpha_i \geq 0 \\
            \forall i, \beta_i \geq 0
        }
    } - \frac{1}{2} \vec{\alpha}^T Q \vec{\alpha} + \sum_i \alpha_i
\]

\section{Kernel Trick}
Suppose that our dataset is ``linearly separable'' by another coordinate system (for example, a circle in polar coordinates). \\
Therefore, regarding the question of ``is a dataset linearly separable'', we ask more on ``is there a coordinate system by which a dataset is linearly separable''.
In doing so, we are effectively lifting features. In the case of circles, we are lifting our feature via a feature function of second-order terms:
\[
    \vec{x} = \Phi(\vec{x})
\]
We would thus change the constraint on hyperplane separability of planes in soft-margin SVM into:
\[
    \forall i, y_i (\vec{w}^T \Phi(\vec{x}_i) + b) \geq 1 - \xi_i
\]
This brings us to the question: suppose we lift features to achieve better performance in SVM, how does the computational cost fare, and how do we reduce such cost?

Let us propose the idea of a kernel function:
\[
    \{\Phi(\vec{x}_i)\}^T \Phi(\vec{x}_j) = \mathcal{K}(\vec{x}_i, \vec{x}_j)
\]
The kernel function's purpose is to reduce the computational cost of inner products of lifted features, such that we may compute the matrix $Q$ relatively efficiently. \\
There are many choices of the kernel function $\mathcal{K}$, which we introduce in COMPSCI 189. \\

For those who are interested, I introduce a description of Kernel Trick from COMPSCI 189 below.

\section{Kernel Trick (from COMPSCI 189)}
\subsection{Motivation}
When we have $d$ input features, degree-$p$ polynomial regression presents $\mathcal{O} (d^p)$ features.
This is a lot.
However, there is a magic that allows us to use these features without computing all of them.

Such magical trick is based on two observations that, in every learning algorithm:
\begin{enumerate}
    \item The weights can be written as a linear combination of the sample points:
    \[
        w = X^T a = \sum_{i = 1}^n a_i X_i, a \in \R^n
    \]
    \item We can use inner products of $\Phi(x)$ (lifted features) only, and not have to compute the lifted features themselves. These inner products can be computed rather efficiently.
\end{enumerate}
For observation (1), we would substitute the above expression of $w$ into an objective function, and now optimize within the algorithm with respect to $a$ (instead of to $w$).

\subsection{Kernel Ridge Regression}
We may first center $X$ and $y$ (without interfering with the fictitious dimension) such that they are zero-meaned. \\
This lets us replace $I'$ (identity matrix that doesn't punish fictitious dimension) with $I$ in normal equations:
\[
    (X^T X + \lambda I) w = X^T y
\]
Then, suppose $a$ of the kernel trick is a solution to:
\[
    (X^T X + \lambda I) a = y
\]
we obtain:
\[
    X^T y = (X^T X + \lambda I) X^T a
\]
such that $w = X^T a$ is a solution to the normal equations, and $w$ is a linear combo of sample points. \\
Here, then $a$ is a dual solution that solves the dual form of ridge regression:
\[
    \min_w \pnorm[2]{X X^T a - y}{} + \lambda \pnorm[2]{X^T w}{}
\]
When we train, we solve for the $a$ that suits the training set.
Then, when we test, we compute the regression function as:
\[
    h(z) = w^T z = a^T X z = \sum_{i = 1}^n a_i (X_i^T z)
\]
Here, we define a kernel function to speed up the computation. \\
Let $k(x, z) = x^T z$ be a kernel function, then let the kernel matrix be defined as $K = X X^T$. \\
In other words, $K_{i,j} = k(X_i, X_j)$.

The dual ridge regression algorithm thus involves the following steps:
\begin{bindenum}
    \item $\forall i, j, K_{i,j} \leftarrow k(X_i, X_j)$ (This is $\mathcal{O}(n^2 d)$ times)
    \item Solve $(K + \lambda I)a = y$ for $a$ (This is $\mathcal{O}(n^3)$ times)
    \item For each test point $z$, $h(z) \leftarrow \sum_i a_i k(X_i, z)$ (This is $\mathcal{O}(n d)$ times)
\end{bindenum}
Here, we see that the dual algorithm solves an $n \times n$ lnear system with $\mathcal{O} (n^3 + n^2 d)$ time.
The primal algorithm, on the other hand, sovles a $d \times d$ linear system with $\mathcal{O} (d^3 + d^2 n)$ time.
We prefer dual when $d > n$.

\subsection{The Kernel Trick (Kernelization), Polynomial}
The polynomial kernel function of degree-$p$ polynomial is:
\[
    k(x, z) = {(x^T z + 1)}^p
\]
Essentially,
\[
    {(x^T z + 1)}^p = {\Phi(x)}^T \Phi(z)
\]
where, $\Phi(x)$ contains every monomial in $X$ of degree $o, \dots, p$.

\textbf{Example.}
For $d = 2$, $p = 2$,
\begin{align*}
    {(x^T z + 1)}^2
    &= x_1^2 z_1^2 + x_2^2 z_2^2 + 2 x_1 z_1 x_2 z_2 + 2 x_1 z_1 + 2 x_2 z_2 + 1 \\
    &= \begin{bmatrix} x_1^2 & x_2^2 & \sqrt{2} x_1 x_2 & \sqrt{2} x_1 & \sqrt{2} x_2 & 1 \end{bmatrix}
    \begin{bmatrix} z_1^2 \\ z_2^2 \\ \sqrt{2} z_1 z_2 \\ \sqrt{2} z_1 \\ \sqrt{2} z_2 \\ 1 \end{bmatrix} \\
    &= {\Phi(x)}^T \Phi(z)
\end{align*}
Here, we have computed ${\Phi(x)}^T \Phi(z)$ in $\mathcal{O}(d)$ time instead of $\mathcal{O}(d^p)$.
Therefore, kerneled ridge regression undderlyingly works with lifted feature vectors instead of feature vectors per se, buy we use the polynomial kernel function such that we do not need to compute the feature vectors themselves.
